# kubernetes

本文档是kubernetes1.16.3kubeadm安装kubernetes使用外部etcd集群的第六篇4

### [上一篇 下载kubernetes工具](https://github.com/mytting/kubernetes/blob/master/A-kubeadm%E5%AE%89%E8%A3%85Kubernetes%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E5%A4%96%E9%83%A8etcd/v1.16.3-E%20%E4%B8%8B%E8%BD%BDkubernetes%E5%B7%A5%E5%85%B7.md)

初始化集群

## 1 配置自动初始化集群配置文件

配置环境变量

按照的版本控制变量

```
export version=$(kubeadm version  | awk -F, '{print $3}' | awk -F: '{print $2}' | awk -F\" '{print $2}')
```

主节点IP地址变量（如果有多个master，也可以指向代理，然后代理器代理后端节点）

这里指定192.168.10.10，请根据自己环境修改

```
echo \"192.168.10.10  apiserver.demo\" >> /etc/hosts
```

编辑配置文件

```
cat <<EOF > kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
kubernetesVersion: $version
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
controlPlaneEndpoint: "192.168.10.10:6443"
networking:
  podSubnet: "10.244.0.0/16"
etcd:
    external:
        endpoints:
        - https://$MASTER_IP:2379
        - https://$NODE1_IP:2379
        - https://$NODE2_IP:2379
        caFile: /usr/local/etcd/ssl/ca.pem
        certFile: /usr/local/etcd/ssl/etcd.pem
        keyFile: /usr/local/etcd/ssl/etcd-key.pem
EOF
```

解释

- kubernetesVersion: $version：按照的版本，环境变量为上面定义的
- imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers，指定国内镜像仓库地址
- controlPlaneEndpoint: "apiserver.demo:6443"：主节点地址
-   podSubnet: "10.244.0.0/16"：容器网段
- 下面为etcd的证书和IP

初始化集群

```
kubeadm init --config=kubeadm-config.yaml
```

输出信息

```
W1126 21:37:50.285446   11800 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
W1126 21:37:50.285533   11800 version.go:102] falling back to the local client version: v1.16.3
[init] Using Kubernetes version: v1.16.3
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 19.03.2. Latest validated version: 18.09
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.10 192.168.10.10]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 16.005940 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.16" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node master as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 6ldwab.otdc2blhzzy9ee9a
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons]: Migrating CoreDNS Corefile
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities 
and service account keys on each node and then running the following as root:

  kubeadm join 192.168.10.10:6443 --token 6ldwab.otdc2blhzzy9ee9a \
    --discovery-token-ca-cert-hash sha256:18c5ec903bed7e06fba0fcd81d1974d2652b992d3c7fea9875e895169a86717d \
    --control-plane 	  

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.10.10:6443 --token 6ldwab.otdc2blhzzy9ee9a \
    --discovery-token-ca-cert-hash sha256:18c5ec903bed7e06fba0fcd81d1974d2652b992d3c7fea9875e895169a86717d 
```

第一条为主节点加入集群需要的token

第二条为node节点加入集群需要的token

两条token24小时内有效

然后需要给kubectl命令配置证书

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### [下一篇 从节点加入集群](https://github.com/mytting/kubernetes/blob/master/A-kubeadm%E5%AE%89%E8%A3%85Kubernetes%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E5%A4%96%E9%83%A8etcd/v1.16.3-G%20%E4%BB%8E%E8%8A%82%E7%82%B9%E5%8A%A0%E5%85%A5%E9%9B%86%E7%BE%A4.md)