# kubernetes

本文档是kubernetes1.16.1二进制安装的第八篇

## 注意 本文所有操作所有节点执行

### [上一篇 RBAC授权](https://github.com/mytting/kubernetes/blob/master/A-%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85Kubernetes/v1.16.1-G%20RBAC%E6%8E%88%E6%9D%83.md)

本文主要介绍部署工作节点的各个组件

 本部分将会部署 Kubernetes 工作节点。每个节点上将会安装以下服务：
container networking plugins
kubelet
kube-proxy



安装依赖
安装 OS 依赖组件：

```
yum install -y socat conntrack ipset
```

socat 命令用于支持 kubectl port-forward 命令。

下载worker 二进制文件

```
wget --timestamping \
 https://github.com/containernetworking/plugins/releases/download/v0.8.2/cni-plugins-linux-amd64-v0.8.2.tgz \
  https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.17.3/bin/linux/amd64/kubelet
```

 安装 worker 二进制文件

```
chmod +x kubectl kube-proxy kubelet
sudo mv kubectl kube-proxy kubelet /usr/local/bin/
```

解压cni插件

```
tar -zxvf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin/
```

生成kubelet.service systemd服务文件

```
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
   --config=/var/lib/kubelet/kubelet-config.yaml \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --pod-infra-container-image=cargo.caicloud.io/caicloud/pause-amd64:3.1 \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2 \\
  --container-runtime=docker \\
  --container-runtime-endpoint=unix:///var/run/dockershim.sock \\
  --image-pull-progress-deadline=15m

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

**配置参数详解**

| 配置选项                        | 选项说明                                                     |
| ------------------------------- | ------------------------------------------------------------ |
| --bootstrap-kubeconfig          | 指定令牌认证文件                                             |
| --cert-dir                      | 设置kube-controller-manager生成证书和私钥的目录              |
| --cni-conf-dir=                 | 指定cni配置文件目录                                          |
| --container-runtime=docker      | 指定容器运行时引擎                                           |
| --container-runtime-endpoint=   | 监听的unix socket位置（Windows上面为 tcp 端口）。            |
| --root-dir=                     | kubelet 保存数据的目录，默认：/var/lib/kubelet               |
| --kubeconfig=                   | kubelet作为客户端使用的kubeconfig认证文件，此文件是由kube-controller-mananger生成的 |
| --config=                       | 指定kubelet配置文件                                          |
| --hostname-override=            | 用来配置该节点在集群中显示的主机名，kubelet设置了-–hostname-override参数后，kube-proxy也需要设置，否则会出现找不到Node的情况 |
| --pod-infra-container-image=    | 每个pod中的network/ipc 名称空间容器将使用的镜像              |
| --image-pull-progress-deadline= | 镜像拉取进度最大时间，如果在这段时间拉取镜像没有任何进展，将取消拉取，默认：1m0s |
| --volume-plugin-dir=            | 第三方卷插件的完整搜索路径，默认："/usr/libexec/kubernetes/kubelet-plugins/volume/exec/" |
| --logtostderr=true              | 日志记录到标准错误而不是文件                                 |
| --v=2                           | 日志级别详细程度的数字                                       |

master节点

```
cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.250.0.10"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/master.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/master-key.pem"
address: "$MASTER_IP"
staticPodPath: ""
syncFrequency: 1m
fileCheckFrequency: 20s
httpCheckFrequency: 20s
staticPodURL: ""
port: 10250
readOnlyPort: 0
rotateCertificates: true
serverTLSBootstrap: true
registryPullQPS: 0
registryBurst: 20
eventRecordQPS: 0
eventBurst: 20
enableDebuggingHandlers: true
enableContentionProfiling: true
healthzPort: 10248
healthzBindAddress: "$MASTER_IP"
nodeStatusUpdateFrequency: 10s
nodeStatusReportFrequency: 1m
imageMinimumGCAge: 2m
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
volumeStatsAggPeriod: 1m
kubeletCgroups: ""
systemCgroups: ""
cgroupRoot: ""
cgroupsPerQOS: true
cgroupDriver: cgroupfs
runtimeRequestTimeout: 10m
hairpinMode: promiscuous-bridge
maxPods: 220
podCIDR: "10.244.0.0/16"
podPidsLimit: -1
resolvConf: /etc/resolv.conf
maxOpenFiles: 1000000
kubeAPIQPS: 1000
kubeAPIBurst: 2000
serializeImagePulls: false
evictionHard:
  memory.available: "100Mi"
  nodefs.available: "10%"
  nodefs.inodesFree: "5%"
  imagefs.available: "15%"
evictionSoft: {}
enableControllerAttachDetach: true
failSwapOn: true
containerLogMaxSize: 20Mi
containerLogMaxFiles: 10
systemReserved: {}
kubeReserved: {}
systemReservedCgroup: ""
kubeReservedCgroup: ""
enforceNodeAllocatable: ["pods"]
EOF
```

node1 节点的配置文件  在master主机上写入文件

```
cat <<EOF | sudo tee node1.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.250.0.10"
runtimeRequestTimeout: "15m"
address: "$NODE1_IP"
staticPodPath: ""
syncFrequency: 1m
fileCheckFrequency: 20s
httpCheckFrequency: 20s
staticPodURL: ""
port: 10250
readOnlyPort: 0
rotateCertificates: true
serverTLSBootstrap: true
registryPullQPS: 0
registryBurst: 20
eventRecordQPS: 0
eventBurst: 20
enableDebuggingHandlers: true
enableContentionProfiling: true
healthzPort: 10248
healthzBindAddress: "$NODE1_IP"
nodeStatusUpdateFrequency: 10s
nodeStatusReportFrequency: 1m
imageMinimumGCAge: 2m
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
volumeStatsAggPeriod: 1m
kubeletCgroups: ""
systemCgroups: ""
cgroupRoot: ""
cgroupsPerQOS: true
cgroupDriver: cgroupfs
runtimeRequestTimeout: 10m
hairpinMode: promiscuous-bridge
maxPods: 220
podCIDR: "10.244.0.0/16"
podPidsLimit: -1
resolvConf: /etc/resolv.conf
maxOpenFiles: 1000000
kubeAPIQPS: 1000
kubeAPIBurst: 2000
serializeImagePulls: false
evictionHard:
  memory.available: "100Mi"
  nodefs.available: "10%"
  nodefs.inodesFree: "5%"
  imagefs.available: "15%"
evictionSoft: {}
enableControllerAttachDetach: true
failSwapOn: true
containerLogMaxSize: 20Mi
containerLogMaxFiles: 10
systemReserved: {}
kubeReserved: {}
systemReservedCgroup: ""
kubeReservedCgroup: ""
enforceNodeAllocatable: ["pods"]
tlsCertFile: "/var/lib/kubelet/node1.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/node1-key.pem"
EOF
```

```
scp node1.yaml node1:/var/lib/kubelet/kubelet-config.yaml
rm -rf node1.yaml
```

node2节点

```
cat <<EOF | sudo tee node2.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.250.0.10"
runtimeRequestTimeout: "15m"
address: "$NODE2_IP"
staticPodPath: ""
syncFrequency: 1m
fileCheckFrequency: 20s
httpCheckFrequency: 20s
staticPodURL: ""
port: 10250
readOnlyPort: 0
rotateCertificates: true
serverTLSBootstrap: true
registryPullQPS: 0
registryBurst: 20
eventRecordQPS: 0
eventBurst: 20
enableDebuggingHandlers: true
enableContentionProfiling: true
healthzPort: 10248
healthzBindAddress: "$NODE2_IP"
nodeStatusUpdateFrequency: 10s
nodeStatusReportFrequency: 1m
imageMinimumGCAge: 2m
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
volumeStatsAggPeriod: 1m
kubeletCgroups: ""
systemCgroups: ""
cgroupRoot: ""
cgroupsPerQOS: true
cgroupDriver: cgroupfs
runtimeRequestTimeout: 10m
hairpinMode: promiscuous-bridge
maxPods: 220
podCIDR: "10.244.0.0/16"
podPidsLimit: -1
resolvConf: /etc/resolv.conf
maxOpenFiles: 1000000
kubeAPIQPS: 1000
kubeAPIBurst: 2000
serializeImagePulls: false
evictionHard:
  memory.available: "100Mi"
  nodefs.available: "10%"
  nodefs.inodesFree: "5%"
  imagefs.available: "15%"
evictionSoft: {}
enableControllerAttachDetach: true
failSwapOn: true
containerLogMaxSize: 20Mi
containerLogMaxFiles: 10
systemReserved: {}
kubeReserved: {}
systemReservedCgroup: ""
kubeReservedCgroup: ""
enforceNodeAllocatable: ["pods"]
tlsCertFile: "/var/lib/kubelet/node2.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/node2-key.pem"
EOF
```

```
scp node2.yaml node2:/var/lib/kubelet/kubelet-config.yaml
rm -rf node2.yaml
```

**配置详解**

| 配置选项                                    | 选项说明                                                     |
| ------------------------------------------- | ------------------------------------------------------------ |
| address                                     | kubelet 服务监听的地址                                       |
| staticPodPath: ""                           | kubelet 会定期的扫描这个文件夹下的YAML/JSON 文件来创建/删除静态Pod，使用kubeadm安装时非常有用 |
| syncFrequency: 1m                           | 同步运行容器和配置之间的最大时间间隔，默认为1m               |
| fileCheckFrequency: 20s                     | 检查新数据配置文件的周期，默认 20s                           |
| httpCheckFrequency: 20s                     | 通过 http 检查新数据的周期，默认 20s                         |
| staticPodURL: ""                            |                                                              |
| port: 10250                                 | kubelet 服务的端口，默认 10250                               |
| readOnlyPort: 0                             | 没有认证/授权的只读 kubelet 服务端口 ，设置为 0 表示禁用，默认 10255 |
| rotateCertificates                          | 远程认证，默认为false                                        |
| serverTLSBootstrap: true                    | kubelet安全引导认证，重启 Kubelet，会发现出现了新的 CSR      |
| authentication:                             | 认证方式有以下几种                                           |
| anonymous:                                  | 匿名                                                         |
| enabled: false                              | 值为false                                                    |
| webhook:                                    | webhook的方式                                                |
| enabled: true                               | 值为true                                                     |
| x509:                                       | x509认证                                                     |
| clientCAFile: "/etc/kubernetes/cert/ca.pem" | 集群ca证书                                                   |
| authorization:                              | 授权                                                         |
| mode: Webhook                               | 授权webhook                                                  |
| registryPullQPS                             | 限制每秒拉取镜像个数，设置为 0 表示不限制， 默认 5           |
| registryBurst                               | 仅当 --registry-qps 大于 0 时使用，设置拉取镜像的最大并发数，允许同时拉取的镜像数，不能超过 registry-qps ，默认 10 |
| eventRecordQPS                              | 限制每秒创建的事件数目，设置为0则不限制，默认为5             |
| eventBurst                                  | 当--event-qps大于0时，临时允许该事件记录值超过设定值，但不能超过 event-qps 的值，默认10 |
| enableDebuggingHandlers                     | 启用用于日志收集和本地运行容器和命令的服务端端点，默认值：true |
| enableContentionProfiling                   | 如果启用了 profiling，则启用性能分析锁                       |
| healthzPort                                 | 本地 healthz 端点的端口，设置为 0 将禁用，默认值：10248      |
| healthzBindAddress                          | 监听healthz 端口的地址                                       |
| clusterDomain: "cluster.local"              | 集群域名, kubelet 将配置所有容器除了主机搜索域还将搜索当前域 |
| clusterDNS:                                 | DNS 服务器的IP地址列表                                       |
| - "10.254.0.2"                              | 指定一个DNS服务器地址                                        |
| nodeStatusUpdateFrequency: 10s              | node节点状态更新上报频率，默认：10s                          |
| nodeStatusReportFrequency: 1m               | node节点上报自身状态频率，默认：1m                           |
| imageMinimumGCAge: 2m                       | 设置镜像最少多久没有被使用才会被清理                         |
| imageGCHighThresholdPercent: 85             | 设置镜像占用磁盘比率最大值，超过此值将执行镜像垃圾回收，默认 85 |
| imageGCLowThresholdPercent: 80              | 设置镜像占用磁盘比率最小值，低于此值将停止镜像垃圾回收，默认 80 |
| volumeStatsAggPeriod: 1m                    | 指定kubelet计算和缓存所有容器组及卷的磁盘使用量时间间隔，设置为0禁用卷计算，默认：1m |
| kubeletCgroups: ""                          | 可选的 cgroups 的绝对名称来创建和运行 kubelet                |
| systemCgroups: ""                           | 可选的 cgroups 的绝对名称，用于将未包含在 cgroup 内的所有非内核进程放置在根目录 / 中，修改这个参数需要重启 |
| cgroupRoot: ""                              | Pods 可选的root cgroup, 这是由container runtime，在最佳工作的基础上处理的，默认值: ''，意思是使用container runtime的默认处理 |
| cgroupsPerQOS: true                         | 支持创建QoS cgroup的层级结构，如果是true，最高层级的         |
| cgroupDriver: systemd                       | Kubelet用来操作主机cgroups的驱动                             |
| runtimeRequestTimeout: 10m                  | 除了长时间运行的请求(pull,logs,exec，attach)以外，所有runtime请求的超时时间，当触发超时时，kubelet会取消请求，抛出一个错误并稍后重试，默认值:2m0s |
| hairpinMode: promiscuous-bridge             | Kubelet该怎么设置hairpin promiscuous-bridge.这个参数允许Service的端点试图访问自己的Service。如果网络没有正确配置为 “hairpin” 流量，通常当 kube-proxy 以 iptables 模式运行，并且 Pod 与桥接网络连接时，就会发生这种情况。Kubelet 公开了一个 hairpin-mode 标志，如果 pod 试图访问他们自己的 Service VIP，就可以让 Service 的 endpoints 重新负载到他们自己身上。hairpin-mode 标志必须设置为 hairpin-veth 或者 promiscuous-bridge。 |
| maxPods                                     | 当前 kubelet 可以运行的容器组数目，默认：110                 |
| podCIDR                                     | pod使用的CIDR网段                                            |
| podPidsLimit: -1                            | pod中设置PID限制                                             |
| resolvConf                                  | 用作容器DNS解析配置的解析器配置文件，默认： "/etc/resolv.conf" |
| maxOpenFiles: 1000000                       | kubelet 进程可以打开的文件数目，默认：1000000                |
| kubeAPIQPS                                  | 与kube-apiserver会话时的QPS，默认：15                        |
| kubeAPIBurst                                | 与kube-apiserver会话时的并发数，默认：10                     |
| serializeImagePulls: false                  | 禁止一次只拉取一个镜像                                       |
| evictionHard:                               | 一个清理阈值的集合，达到该阈值将触发一次容器清理             |
| memory.available: "100Mi"                   | 小gf 100Mi时清理                                             |
| nodefs.available: "10%"                     |                                                              |
| nodefs.inodesFree: "5%"                     |                                                              |
| imagefs.available: "15%"                    |                                                              |
| evictionSoft: {}                            | 清理阈值的集合，如果达到一个清理周期将触发一次容器清理       |
| enableControllerAttachDetach: true          | 允许附加/分离控制器来管理调度到当前节点的附加/分离卷，并禁用kubelet执行任何附加/分离操作，默认：true |
| failSwapOn: true                            | 如果在节点上启用了swap，kubelet将启动失败                    |
| containerLogMaxSize: 20Mi                   | 容器日志容量最大值                                           |
| containerLogMaxFiles: 10                    | 容器日志文件数最大值                                         |
| systemReserved: {}                          | https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/，系统守护进程争取资源预留，这里均设置为默认值 |
| kubeReserved: {}                            | kubernetes 系统守护进程争取资源预留，这里均设置为默认值      |
| systemReservedCgroup: ""                    | 要选择性的在系统守护进程上执行 kube-reserved，需要把 kubelet 的 --kube-reserved-cgroup 标志的值设置为 kube 守护进程的父控制组要想在系统守护进程上可选地执行 system-reserved，请指定 --system-reserved-cgroup kubelet 标志的值为 OS 系统守护进程的父级控制组，这里均设置为默认值 |
| kubeReservedCgroup: ""                      | 要选择性的在系统守护进程上执行 kube-reserved，需要把 kubelet 的 --kube-reserved-cgroup 标志的值设置为 kube 守护进程的父控制组这里均设置为默认值 |
| enforceNodeAllocatable: ["pods"]            | 无论何时，如果所有 pod 的总用量超过了 Allocatable，驱逐 pod 的措施将被执行 |

kubelet组件采用主动的查询机制，定期向kube-apiserver获取当前节点应该处理的任务，如果有任务分配到了自己身上（如创建Pod），从而他去处理这些任务；

kubelet暴露了两个端口10248，http形式的healthz服务，另一个是10250，https服务，其实还有一个只读的10255端口，这里是禁用的。

## 生成kube-proxy systemd服务启动文件

本文是二进制安装kubernetes v1.17.0 之kube-proxy，kube-proxy是什么，这里就不得不说下service，service是一组Pod的抽象集合，它相当于一组Pod的负载均衡器，负责将请求分发到对应的pod，kube-proxy就是负责service的实现的，当请求到达service时，它通过label关联到后端并转发到某个Pod；kube-proxy提供了三种负载均衡模式：用户空间、iptables、ipvs，网上有很多关于这三种模式的区别，这里先不详述，本文采用ipvs。



kube-proxy需要运行在所有节点上（因为我们master节点也有Pod，如果没有的话，可以只部署在非master节点上），kube-proxy它主动的去监听kube-apiserver中service和endpoint的变化情况，然后根据定义的模式，创建路由规则，并提供服务service IP（headless类型的service无IP）和负载均衡功能。注意：在所有节点安装ipvsadm和ipset命令，加载ip_vs内核模块，准备章节已经执行过。

master节点上操作

```
cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  burst: 200
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
  qps: 100
bindAddress: $MASTER_IP
healthzBindAddress: $MASTER_IP:10256
metricsBindAddress: $MASTER_IP:10249
enableProfiling: true
clusterCIDR: 10.244.0.0/16
mode: "ipvs"
portRange: ""
kubeProxyIPTablesConfiguration:
  masqueradeAll: false
kubeProxyIPVSConfiguration:
  scheduler: rr
  excludeCIDRs: []
EOF

cat <<EOF | sudo tee node1.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  burst: 200
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
  qps: 100
bindAddress: $NODE1_IP
healthzBindAddress: $NODE1_IP:10256
metricsBindAddress: $NODE1_IP:10249
enableProfiling: true
clusterCIDR: 10.244.0.0/16
mode: "ipvs"
portRange: ""
kubeProxyIPTablesConfiguration:
  masqueradeAll: false
kubeProxyIPVSConfiguration:
  scheduler: rr
  excludeCIDRs: []
EOF
scp node1.yaml node1:/var/lib/kube-proxy/kube-proxy-config.yaml
rm -rf node1.yaml

cat <<EOF | sudo tee node2.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  burst: 200
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
  qps: 100
bindAddress: $NODE2_IP
healthzBindAddress: $NODE2_IP:10256
metricsBindAddress: $NODE2_IP:10249
enableProfiling: true
clusterCIDR: 10.244.0.0/16
mode: "ipvs"
portRange: ""
kubeProxyIPTablesConfiguration:
  masqueradeAll: false
kubeProxyIPVSConfiguration:
  scheduler: rr
  excludeCIDRs: []
EOF
scp node2.yaml node2:/var/lib/kube-proxy/kube-proxy-config.yaml
rm -rf node2.yaml
```

**配置详解**



| 配置选项                        | 选项说明                                                     |
| ------------------------------- | ------------------------------------------------------------ |
| clientConnection                | 与kube-apiserver交互时的参数设置                             |
| burst: 200                      | 临时允许该事件记录值超过qps设定值                            |
| kubeconfig                      | kube-proxy 客户端连接 kube-apiserver 的 kubeconfig 文件路径设置 |
| qps: 100                        | 与kube-apiserver交互时的QPS，默认值5                         |
| bindAddress                     | kube-proxy监听地址                                           |
| healthzBindAddress              | 用于检查服务的IP地址和端口，默认：0.0.0.0:10256              |
| metricsBindAddress              | metrics服务的ip地址和端口，默认：127.0.0.1:10249             |
| enableProfiling                 | 如果设为true，则通过/debug/pprof处理程序上的web界面进行概要分析 |
| clusterCIDR                     | kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT |
| hostnameOverride                | 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则； |
| mode                            | 使用 ipvs 模式                                               |
| portRange                       | 主机端口的范围(beginPort- endport，单端口或beginPort+偏移量)，可用于代理服务流量。如果(未指定，0，或0-0)端口将被随机选择 |
| kubeProxyIPTablesConfiguration: |                                                              |
| masqueradeAll: false            | 如果使用纯iptables代理，SNAT所有通过服务集群ip发送的通信     |
| kubeProxyIPVSConfiguration:     |                                                              |
| scheduler: rr                   | 当proxy为ipvs模式时，ipvs调度类型                            |
| excludeCIDRs: []                |                                                              |



```
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

启动worker服务

```
systemctl daemon-reload
systemctl enable kubelet kube-proxy
systemctl restart kubelet kube-proxy
```

此时所有节点的kubelet启动之后 会自动加入集群

查看集群节点

```
kubectl get nodes
```

输出信息

```
NAME     STATUS     ROLES    AGE   VERSION
master   NotReady   <none>   9s    v1.16.1
node1    NotReady   <none>   39m   v1.16.1
node2    NotReady   <none>   39m   v1.16.1
```

可以看到 所有的节点都已经成功加入集群

状态显示 NotReady 是因为没有配置网络插件 配置好网络插件就 Ready了

### [下一篇部署插件](https://github.com/mytting/kubernetes/blob/master/A-%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85Kubernetes/v1.16.1-I%20%E9%83%A8%E7%BD%B2%E6%8F%92%E4%BB%B6.md)

